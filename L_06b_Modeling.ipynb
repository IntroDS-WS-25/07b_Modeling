{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiQGo4Gfydzh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Introduction to Data Science*\n",
    "\n",
    "# Modeling: Part b)\n",
    "\n",
    "Gunther Gust<br>\n",
    "Chair of Enterprise AI\n",
    "\n",
    "Winter Semester 25/26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6YntqqTydzi"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/d3.png?raw=1\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FmJUl6kydzi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/ao_modeling.png\" style=\"width:80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_xOz6Kkydzi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* Predictive Models\n",
    "    - Random Forests\n",
    "    - Boosting Methods\n",
    "* Unsupervised Learning\n",
    "    - k-Means Clustering\n",
    "    - Other clustering techniques\n",
    "\n",
    "Credits: The implementation data examples are adopted from www.kaggle.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1nxE08gydzv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHfrMkflydzv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision Trees (CART) are:\n",
    "- Easy to interpret  \n",
    "- Flexible  \n",
    "- Able to model nonlinear relationships  \n",
    "\n",
    "But they also have a major weakness:\n",
    "- __They have extremely high variance:__\n",
    "    - Small changes in the training data → very different trees  \n",
    "    - Deep trees can perfectly fit the training data → **overfitting**  \n",
    "    \n",
    "High variance harms generalization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In contrast, a shallow tree has **lower variance** but **high bias** (underfitting, the model is \"biased\" in certain regions of the data)  \n",
    "\n",
    "It is hard to tune a single tree to achieve __both low bias *and* low variance__, but:\n",
    "\n",
    "> How about combining many (smaller) trees and average their predictions to reduce variance?\n",
    "\n",
    "This is exactly what __random forests:__ do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cM_gODLydzv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/03/Random_forest_explain.png?raw=1\" style=\"width:80%; float:left;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "A Random Forest is an **ensemble** of decision trees.\n",
    "\n",
    "Key idea:\n",
    "- “Many weak learners can create a strong learner.”\n",
    "\n",
    "Benefit:\n",
    "- Individual errors tend to cancel out.\n",
    "- Forest becomes more stable and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Why Ensembles Improve Accuracy\n",
    "\n",
    "**Task:** Email classification (SPAM / NOT SPAM)\n",
    "\n",
    "- We have **3 uncorrelated models**, each with accuracy **70%**.\n",
    "- The ensemble prediction is made by **majority vote** of the 3 models:\n",
    "\n",
    "\n",
    "| Outputs of three models     | Probability                        | Ensemble’s output |\n",
    "|-----------------------------|-------------------------------------|-------------------|\n",
    "| All three are correct       | `0.7 * 0.7 * 0.7 = 0.343`           | Correct           |\n",
    "| Only two are correct        | `(0.7 * 0.7 * 0.3) * 3 = 0.441`     | Correct           |\n",
    "| Only one is correct         | `(0.3 * 0.3 * 0.7) * 3 = 0.189`     | Wrong             |\n",
    "| None are correct            | `0.3 * 0.3 * 0.3 = 0.027`           | Wrong             |\n",
    "\n",
    "\n",
    "- Probability that at least **2 models** are correct:\n",
    "\n",
    "        0.343 + 0.441 = 0.784 \n",
    "        \n",
    "- Ensembling __improves accuracy by 8.4%__ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Important note:__ The example does not take into account the fact that in real-world applications individial learners are __never perfectly uncorrelated__ - which reduces the gains of ensembling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training: Bagging (Bootstrap Aggregation)\n",
    "\n",
    "1. __Bootstrapping__ for building the trees:\n",
    "    1. Draw a **bootstrap sample** of observations from the training data (= sample with replacement)\n",
    "    2. Train a CART tree on that sample\n",
    "    3. Repeat for many trees\n",
    "\n",
    "> __Effect:__ Each tree sees __*slightly different data*__ and their predictions errors are __*less correlated*__\n",
    "\n",
    "2. __Aggregation__ for predictions:\n",
    "- **Classification:** majority vote of all trees  \n",
    "- **Regression:** average of predictions of all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Feature Selection (Feature Bagging)\n",
    "\n",
    "In addition to bagging, Random Forests randomize the **feature selection** at each split.\n",
    "\n",
    "At each node:\n",
    "- Consider only a **random subset** of the overall p features  \n",
    "  - Classification: often `sqrt(p)`  \n",
    "  - Regression: often `p/3`\n",
    "\n",
    "Benefits:\n",
    "- Trees become more **decorrelated**  \n",
    "- No single strong predictor dominates the forest  \n",
    "- Improves generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "### Model Size\n",
    "- `n_estimators`: number of trees  \n",
    "- More trees → better performance (to a point)\n",
    "\n",
    "### Randomization\n",
    "- `max_features`: number of features considered at each split  \n",
    "- `bootstrap`: whether bootstrapping for the trainings samples is used (usually True)\n",
    "\n",
    "### Sampling\n",
    "- `max_samples`: size of bootstrap sample  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/kfoldcv.png\" style=\"width:70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "Random Forests provide feature importance estimates:\n",
    "\n",
    "### 1. Mean Decrease Impurity (MDI)\n",
    "- Based on how much each feature reduces impurity across splits.\n",
    "\n",
    "### 2. Permutation Importance\n",
    "- Shuffle the values of each feature and measure performance drop.\n",
    "- More reliable and intuitive.\n",
    "\n",
    "> If shuffling a feature hurts the model, it must be important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strengths of Random Forests\n",
    "\n",
    "- __Very important:__ High accuracy with minimal tuning  \n",
    "- Robust against overfitting  \n",
    "- Handles nonlinearities and feature interactions  \n",
    "- Works well with mixed data types  \n",
    "- Provides feature importance  \n",
    "- No need for feature scaling  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weaknesses of Random Forests\n",
    "\n",
    "- Less interpretable than a single tree  \n",
    "- Large models (memory-heavy)  \n",
    "- Slower predictions than individual trees  \n",
    "- Struggles with very high-dimensional sparse data (e.g., text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTNnYuetydzv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing a Random Forest\n",
    "Thanks to our Scikit-learn modeling pipeline we can reuse most of our code to train a random forest model with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HB5qY1UQydzk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "Z3Of11zcydzl",
    "outputId": "ef44321a-05f1-49f1-8c1f-e705b182afc6",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Suburb",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Address",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Rooms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SellerG",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Postcode",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Bedroom2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Bathroom",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Car",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Landsize",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BuildingArea",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "YearBuilt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CouncilArea",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Lattitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longtitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Regionname",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Propertycount",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "27af2433-e11d-4126-b455-41b7e1287a25",
       "rows": [
        [
         "0",
         "0",
         "Abbotsford",
         "85 Turner St",
         "2",
         "h",
         "1480000.0",
         "S",
         "Biggin",
         "3/12/2016",
         "2.5",
         "3067.0",
         "2.0",
         "1.0",
         "1.0",
         "202.0",
         null,
         null,
         "Yarra",
         "-37.7996",
         "144.9984",
         "Northern Metropolitan",
         "4019.0"
        ],
        [
         "1",
         "1",
         "Abbotsford",
         "25 Bloomburg St",
         "2",
         "h",
         "1035000.0",
         "S",
         "Biggin",
         "4/02/2016",
         "2.5",
         "3067.0",
         "2.0",
         "1.0",
         "0.0",
         "156.0",
         "79.0",
         "1900.0",
         "Yarra",
         "-37.8079",
         "144.9934",
         "Northern Metropolitan",
         "4019.0"
        ],
        [
         "2",
         "2",
         "Abbotsford",
         "5 Charles St",
         "3",
         "h",
         "1465000.0",
         "SP",
         "Biggin",
         "4/03/2017",
         "2.5",
         "3067.0",
         "3.0",
         "2.0",
         "0.0",
         "134.0",
         "150.0",
         "1900.0",
         "Yarra",
         "-37.8093",
         "144.9944",
         "Northern Metropolitan",
         "4019.0"
        ],
        [
         "3",
         "3",
         "Abbotsford",
         "40 Federation La",
         "3",
         "h",
         "850000.0",
         "PI",
         "Biggin",
         "4/03/2017",
         "2.5",
         "3067.0",
         "3.0",
         "2.0",
         "1.0",
         "94.0",
         null,
         null,
         "Yarra",
         "-37.7969",
         "144.9969",
         "Northern Metropolitan",
         "4019.0"
        ],
        [
         "4",
         "4",
         "Abbotsford",
         "55a Park St",
         "4",
         "h",
         "1600000.0",
         "VB",
         "Nelson",
         "4/06/2016",
         "2.5",
         "3067.0",
         "3.0",
         "1.0",
         "2.0",
         "120.0",
         "142.0",
         "2014.0",
         "Yarra",
         "-37.8072",
         "144.9941",
         "Northern Metropolitan",
         "4019.0"
        ]
       ],
       "shape": {
        "columns": 22,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Date</th>\n",
       "      <th>Distance</th>\n",
       "      <th>...</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>85 Turner St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1480000.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>3/12/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7996</td>\n",
       "      <td>144.9984</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>25 Bloomburg St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/02/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>5 Charles St</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>1465000.0</td>\n",
       "      <td>SP</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/03/2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>40 Federation La</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>PI</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/03/2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7969</td>\n",
       "      <td>144.9969</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>55a Park St</td>\n",
       "      <td>4</td>\n",
       "      <td>h</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>VB</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>4/06/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8072</td>\n",
       "      <td>144.9941</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Suburb           Address  Rooms Type      Price Method  \\\n",
       "0           0  Abbotsford      85 Turner St      2    h  1480000.0      S   \n",
       "1           1  Abbotsford   25 Bloomburg St      2    h  1035000.0      S   \n",
       "2           2  Abbotsford      5 Charles St      3    h  1465000.0     SP   \n",
       "3           3  Abbotsford  40 Federation La      3    h   850000.0     PI   \n",
       "4           4  Abbotsford       55a Park St      4    h  1600000.0     VB   \n",
       "\n",
       "  SellerG       Date  Distance  ...  Bathroom  Car  Landsize  BuildingArea  \\\n",
       "0  Biggin  3/12/2016       2.5  ...       1.0  1.0     202.0           NaN   \n",
       "1  Biggin  4/02/2016       2.5  ...       1.0  0.0     156.0          79.0   \n",
       "2  Biggin  4/03/2017       2.5  ...       2.0  0.0     134.0         150.0   \n",
       "3  Biggin  4/03/2017       2.5  ...       2.0  1.0      94.0           NaN   \n",
       "4  Nelson  4/06/2016       2.5  ...       1.0  2.0     120.0         142.0   \n",
       "\n",
       "   YearBuilt  CouncilArea  Lattitude Longtitude             Regionname  \\\n",
       "0        NaN        Yarra   -37.7996   144.9984  Northern Metropolitan   \n",
       "1     1900.0        Yarra   -37.8079   144.9934  Northern Metropolitan   \n",
       "2     1900.0        Yarra   -37.8093   144.9944  Northern Metropolitan   \n",
       "3        NaN        Yarra   -37.7969   144.9969  Northern Metropolitan   \n",
       "4     2014.0        Yarra   -37.8072   144.9941  Northern Metropolitan   \n",
       "\n",
       "   Propertycount  \n",
       "0         4019.0  \n",
       "1         4019.0  \n",
       "2         4019.0  \n",
       "3         4019.0  \n",
       "4         4019.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melbourne_file_path = 'https://raw.githubusercontent.com/vhaus63/ids_data/refs/heads/main/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "melbourne_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nKd74XOydzl",
    "outputId": "f901843e-3d7f-494a-9894-fe5e2db259b6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 0     13580 non-null  int64  \n",
      " 1   Suburb         13580 non-null  object \n",
      " 2   Address        13580 non-null  object \n",
      " 3   Rooms          13580 non-null  int64  \n",
      " 4   Type           13580 non-null  object \n",
      " 5   Price          13580 non-null  float64\n",
      " 6   Method         13580 non-null  object \n",
      " 7   SellerG        13580 non-null  object \n",
      " 8   Date           13580 non-null  object \n",
      " 9   Distance       13580 non-null  float64\n",
      " 10  Postcode       13580 non-null  float64\n",
      " 11  Bedroom2       13580 non-null  float64\n",
      " 12  Bathroom       13580 non-null  float64\n",
      " 13  Car            13518 non-null  float64\n",
      " 14  Landsize       13580 non-null  float64\n",
      " 15  BuildingArea   7130 non-null   float64\n",
      " 16  YearBuilt      8205 non-null   float64\n",
      " 17  CouncilArea    12211 non-null  object \n",
      " 18  Lattitude      13580 non-null  float64\n",
      " 19  Longtitude     13580 non-null  float64\n",
      " 20  Regionname     13580 non-null  object \n",
      " 21  Propertycount  13580 non-null  float64\n",
      "dtypes: float64(12), int64(2), object(8)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "melbourne_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8S4I6kMydzl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For simplicity we remove rows with missing values for this example. Note that a missing value can sometimes be a valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAwZoy5eydzl",
    "outputId": "3aea9e32-c692-4465-e6be-67888cc0cdd3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13580, 22)\n",
      "(6196, 22)\n"
     ]
    }
   ],
   "source": [
    "print(melbourne_data.shape)\n",
    "melbourne_data.dropna(axis=0, inplace=True)\n",
    "print(melbourne_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aRssVccKydzm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Price",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5bd6e78c-4d47-4b59-9479-33631933b5b1",
       "rows": [
        [
         "count",
         "6196.0"
        ],
        [
         "mean",
         "1068828.202065849"
        ],
        [
         "std",
         "675156.4275015871"
        ],
        [
         "min",
         "131000.0"
        ],
        [
         "25%",
         "620000.0"
        ],
        [
         "50%",
         "880000.0"
        ],
        [
         "75%",
         "1325000.0"
        ],
        [
         "max",
         "9000000.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    6.196000e+03\n",
       "mean     1.068828e+06\n",
       "std      6.751564e+05\n",
       "min      1.310000e+05\n",
       "25%      6.200000e+05\n",
       "50%      8.800000e+05\n",
       "75%      1.325000e+06\n",
       "max      9.000000e+06\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = melbourne_data['Price']\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Jc7d2bjOydzm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea',\n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "19pFwwHPydzm"
   },
   "outputs": [],
   "source": [
    "X = melbourne_data[melbourne_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "jWQtMdTfydzn",
    "outputId": "a8a8d2bc-017b-4ac2-c310-a58b6749b5b7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6196, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Rooms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Bathroom",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Landsize",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BuildingArea",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "YearBuilt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Lattitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longtitude",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "92e3af58-da3d-4bf5-a281-88a0de5a78b3",
       "rows": [
        [
         "1",
         "2",
         "1.0",
         "156.0",
         "79.0",
         "1900.0",
         "-37.8079",
         "144.9934"
        ],
        [
         "2",
         "3",
         "2.0",
         "134.0",
         "150.0",
         "1900.0",
         "-37.8093",
         "144.9944"
        ],
        [
         "4",
         "4",
         "1.0",
         "120.0",
         "142.0",
         "2014.0",
         "-37.8072",
         "144.9941"
        ],
        [
         "6",
         "3",
         "2.0",
         "245.0",
         "210.0",
         "1910.0",
         "-37.8024",
         "144.9993"
        ],
        [
         "7",
         "2",
         "1.0",
         "256.0",
         "107.0",
         "1890.0",
         "-37.806",
         "144.9954"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>-37.8072</td>\n",
       "      <td>144.9941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>-37.8024</td>\n",
       "      <td>144.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>-37.8060</td>\n",
       "      <td>144.9954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rooms  Bathroom  Landsize  BuildingArea  YearBuilt  Lattitude  Longtitude\n",
       "1      2       1.0     156.0          79.0     1900.0   -37.8079    144.9934\n",
       "2      3       2.0     134.0         150.0     1900.0   -37.8093    144.9944\n",
       "4      4       1.0     120.0         142.0     2014.0   -37.8072    144.9941\n",
       "6      3       2.0     245.0         210.0     1910.0   -37.8024    144.9993\n",
       "7      2       1.0     256.0         107.0     1890.0   -37.8060    144.9954"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "aM72_wWaydzn",
    "outputId": "88ecf775-efde-42e6-d50d-9f4d6ad8b655",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "75%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a7abcd73-87f0-4a25-9e6e-588dd3b14c80",
       "rows": [
        [
         "Rooms",
         "6196.0",
         "2.93140735958683",
         "0.9710788063154113",
         "1.0",
         "2.0",
         "3.0",
         "4.0",
         "8.0"
        ],
        [
         "Bathroom",
         "6196.0",
         "1.5763395739186572",
         "0.7113618878731119",
         "1.0",
         "1.0",
         "1.0",
         "2.0",
         "8.0"
        ],
        [
         "Landsize",
         "6196.0",
         "471.00693996126535",
         "897.4498805434331",
         "0.0",
         "152.0",
         "373.0",
         "628.0",
         "37000.0"
        ],
        [
         "BuildingArea",
         "6196.0",
         "141.56864454486765",
         "90.83482372717586",
         "0.0",
         "91.0",
         "124.0",
         "170.0",
         "3112.0"
        ],
        [
         "YearBuilt",
         "6196.0",
         "1964.0819883795998",
         "38.10567344441835",
         "1196.0",
         "1940.0",
         "1970.0",
         "2000.0",
         "2018.0"
        ],
        [
         "Lattitude",
         "6196.0",
         "-37.807904175274366",
         "0.07585043258185697",
         "-38.16492",
         "-37.8554375",
         "-37.80225",
         "-37.7582",
         "-37.45709"
        ],
        [
         "Longtitude",
         "6196.0",
         "144.9902011362169",
         "0.0991646697501218",
         "144.54237",
         "144.9261975",
         "144.9958",
         "145.0527",
         "145.52635"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rooms</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>2.931407</td>\n",
       "      <td>0.971079</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bathroom</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>1.576340</td>\n",
       "      <td>0.711362</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Landsize</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>471.006940</td>\n",
       "      <td>897.449881</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>373.00000</td>\n",
       "      <td>628.0000</td>\n",
       "      <td>37000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BuildingArea</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>141.568645</td>\n",
       "      <td>90.834824</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>124.00000</td>\n",
       "      <td>170.0000</td>\n",
       "      <td>3112.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearBuilt</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>1964.081988</td>\n",
       "      <td>38.105673</td>\n",
       "      <td>1196.00000</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>1970.00000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2018.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lattitude</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>-37.807904</td>\n",
       "      <td>0.075850</td>\n",
       "      <td>-38.16492</td>\n",
       "      <td>-37.855438</td>\n",
       "      <td>-37.80225</td>\n",
       "      <td>-37.7582</td>\n",
       "      <td>-37.45709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longtitude</th>\n",
       "      <td>6196.0</td>\n",
       "      <td>144.990201</td>\n",
       "      <td>0.099165</td>\n",
       "      <td>144.54237</td>\n",
       "      <td>144.926198</td>\n",
       "      <td>144.99580</td>\n",
       "      <td>145.0527</td>\n",
       "      <td>145.52635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count         mean         std         min          25%  \\\n",
       "Rooms         6196.0     2.931407    0.971079     1.00000     2.000000   \n",
       "Bathroom      6196.0     1.576340    0.711362     1.00000     1.000000   \n",
       "Landsize      6196.0   471.006940  897.449881     0.00000   152.000000   \n",
       "BuildingArea  6196.0   141.568645   90.834824     0.00000    91.000000   \n",
       "YearBuilt     6196.0  1964.081988   38.105673  1196.00000  1940.000000   \n",
       "Lattitude     6196.0   -37.807904    0.075850   -38.16492   -37.855438   \n",
       "Longtitude    6196.0   144.990201    0.099165   144.54237   144.926198   \n",
       "\n",
       "                     50%        75%          max  \n",
       "Rooms            3.00000     4.0000      8.00000  \n",
       "Bathroom         1.00000     2.0000      8.00000  \n",
       "Landsize       373.00000   628.0000  37000.00000  \n",
       "BuildingArea   124.00000   170.0000   3112.00000  \n",
       "YearBuilt     1970.00000  2000.0000   2018.00000  \n",
       "Lattitude      -37.80225   -37.7582    -37.45709  \n",
       "Longtitude     144.99580   145.0527    145.52635  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0D-b4cdKydzv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BnN3Bacydzw",
    "outputId": "a1ea2f89-abce-4df1-974a-119faa2ea453",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MAE:     253848.377\n",
      "Random Forest MAE:     193739.859\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Decision Tree Model\n",
    "# -----------------------------\n",
    "tree_model = DecisionTreeRegressor(random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_preds = tree_model.predict(X_valid)\n",
    "tree_mae = mean_absolute_error(y_valid, tree_preds)\n",
    "\n",
    "# -----------------------------\n",
    "# Random Forest Model\n",
    "# -----------------------------\n",
    "forest_model = RandomForestRegressor(\n",
    "    random_state=1,\n",
    "    n_estimators=100\n",
    ")\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_preds = forest_model.predict(X_valid)\n",
    "forest_mae = mean_absolute_error(y_valid, forest_preds)\n",
    "\n",
    "# -----------------------------\n",
    "# Compare performance\n",
    "# -----------------------------\n",
    "print(\"Decision Tree MAE:     {:.3f}\".format(tree_mae))\n",
    "print(\"Random Forest MAE:     {:.3f}\".format(forest_mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is likely room for further improvement, but this is a __big improvement over the decision tree error of 253,000__. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even __without this tuning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, let's use the built-in __feature importance__ method to see which features drive real estate prices in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m importances \u001b[38;5;241m=\u001b[39m forest_model\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Put into a DataFrame for nice display\u001b[39;00m\n\u001b[0;32m      4\u001b[0m feat_importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfeature_names\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m: importances}\n\u001b[0;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(feat_importances[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m], feat_importances[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "importances = forest_model.feature_importances_\n",
    "\n",
    "# Put into a DataFrame for nice display\n",
    "feat_importances = pd.DataFrame(\n",
    "    {\"feature\": feature_names, \"importance\": importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feat_importances[\"feature\"], feat_importances[\"importance\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Excursus: Beyond Feature Importance — Explainable Machine Learning\n",
    "\n",
    "Random Forests give us **feature importance values**, but these have limitations:\n",
    "- They show **how important** a feature is,  \n",
    "- but not **whether it increases or decreases the prediction**,  \n",
    "- and not **how the effect changes across different samples**.\n",
    "\n",
    "There are several __explainability methods__ that can be used with any model and that go further:\n",
    "\n",
    "- **Permutation Importance**: Measures how performance drops when a feature is shuffled.\n",
    " \n",
    "- **Partial Dependence Plots (PDP)**: Show how the *average* prediction changes with a feature.\n",
    "\n",
    "- **Individual Conditional Expectation (ICE)**: Shows how predictions change for *individual samples*.\n",
    "\n",
    "- **LIME (Local Interpretable Model-Agnostic Explanations)**: Explains individual predictions using simple local surrogate models.\n",
    "\n",
    "- **SHAP(SHapley Additive exPlanations)** : \n",
    "    - Shows the **direction** of each feature’s effect (positive / negative),\n",
    "    - Quantifies how much each feature contributes to **individual predictions**,\n",
    "    - Separates **global** and **local** explanations,\n",
    "    - Handles nonlinear interactions naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caS-x4AZydzv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on random forests\n",
    "\n",
    "\n",
    "- For an informal and illustrative introduction, see [this youtube video](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer). \n",
    "- For a formal coverage see [Hastie et al: Elements of Statistical Learning, Chapter 15](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentimeter\n",
    "\n",
    "Let's recap what we have learned on random forests on [Mentimeter](https://www.menti.com/alurte2x2dno). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest: Bagging \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/bagging.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another powerful approach: Boosting\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/boosting.png\" style=\"width:70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Boosting Does (Intuition)\n",
    "\n",
    "Boosting builds a model **sequentially**:\n",
    "\n",
    "1. Train a weak model (usually a very small tree).\n",
    "2. Identify what it got wrong.\n",
    "3. Train the next tree to fix these errors.\n",
    "4. Repeat many times.\n",
    "5. Combine all trees into one strong predictor.\n",
    "\n",
    "**Boosting = “Learning from your mistakes.”**\n",
    "\n",
    "Unlike Random Forests (parallel), boosting trees grow **one after another**,  \n",
    "each improving the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting for regression:\n",
    "\n",
    "- The next learner fits the **residuals** previous learner (remaining errors).\n",
    "- Each step reduces the leftover error.\n",
    "\n",
    "## Boosing for classification:\n",
    "- Misclassified points implicitly get more influence.\n",
    "- Hard examples are emphasized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Adaboost (Adaptive Boosting)\n",
    "\n",
    "- Start with equal weights for all training samples.\n",
    "- After each tree:\n",
    "  - Misclassified samples get **more weight**.\n",
    "  - Correctly classified samples get **less weight**.\n",
    "- The next tree focuses more on \"hard\" examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key parameter: Learning rate\n",
    "\n",
    "In boosting, models are built one after another, and each new model $f_m(x)$ tries to correct the errors of the previous combined model $F_{m-1}(x)$.\n",
    "\n",
    "But we don’t add the full correction — we only add a **fraction** of it. That fraction is the **learning rate**:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\cdot f_m(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $F_m(x)$ = ensemble after adding the m-th tree  \n",
    "- $f_m(x)$ = prediction of the new tree (the “correction”)  \n",
    "- $\\eta$ = learning rate, with $0 < \\eta \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Learning Rate    | Number of Trees  | Risk of Overfitting | Behavior                 |\n",
    "| ---------------- | ---------------- | ------------------- | ------------------------ |\n",
    "| Small (0.01–0.1) | Large (500–2000) | Low                 | Slow, stable learning    |\n",
    "| Medium (0.1–0.3) | Moderate         | Medium              | Good default             |\n",
    "| Large (0.5–1)    | Small            | High                | Jumps too fast, unstable |\n",
    "\n",
    "\n",
    "XGBoost and LightGBM often recommend:\n",
    "\n",
    "- η around 0.05–0.3, and\n",
    "\n",
    "- a lot of trees (hundreds or thousands).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Popular boosting algorithms:\n",
    "\n",
    "\n",
    "\n",
    "| Algorithm     | Key Idea | Strengths | Best Use Case |\n",
    "|---------------|----------|-----------|----------------|\n",
    "| **AdaBoost** | Increase weights of misclassified points | Simple, intuitive | Clean data, simple models |\n",
    "| **Gradient Boosting** | Fit residuals (gradients) | Flexible, foundational | General-purpose boosting |\n",
    "| **XGBoost** | Optimized gradient boosting | Fast, accurate, regularized | Medium/large datasets |\n",
    "| **LightGBM** | Histogram + leaf-wise growth | Very fast, scalable | Very large datasets |\n",
    "| **CatBoost** | Categorical feature handling | Minimal preprocessing | Many categorical variables |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(\n",
    "    n_estimators=1000,      # more trees, but shallow\n",
    "    learning_rate=0.05,    # step size (shrinkage)\n",
    "    max_depth=3,          # depth of individual trees (weak learners)\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "gbr_model.fit(X_train, y_train)\n",
    "gbr_preds = gbr_model.predict(X_valid)\n",
    "gbr_mae = mean_absolute_error(y_valid, gbr_preds)\n",
    "\n",
    "print(\"\\nGradient Boosting MAE: {:.3f}\".format(gbr_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `GradientBoostingRegressor` performs even better than the `Random Forest`. [Scikit-Learn's implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) uses Friedman’s Gradient Boosting algorithm (MART — Multiple Additive Regression Trees), where each stage fits a decision tree to the negative gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just finished training our first machine learning models. To further improve the predictive power of the models we will have to __work on our dataset__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPzD3lmtydzw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing Value Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_rGQVGfydzw",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will start with handling missing values in the data. Most machine learning libraries (including scikit-learn) give an error if we try to build a model using data with missing values. So we'll need to choose a strategy to handle missing values.\n",
    "\n",
    "We have already used a very simple strategy and dropped all rows containing missing values in the first example. To evaluate different approaches we will first load the full dataset and create a train-test split. (Note: As we cannot apply all imputation functions (e.g., mean) to categorical data we will only use numerical predictions in this simple example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H21cvi5_ydzw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# Target variable\n",
    "y = data['Price']\n",
    "\n",
    "# Drop non-numeric variables\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idRiZASAydzw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Imputation using the Mean\n",
    "\n",
    "One popular way to handle missing values is called imputation. Here, we fill in the missing values with some number. For instance, we can fill in the __mean__ value along each column. The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Su_tP-jydzw"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgErmO5bydzw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imputation\n",
    "simple_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(simple_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(simple_imputer.transform(X_valid))\n",
    "\n",
    "# \"Repair\" column names\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvTaFPmuydzw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To evaluate the performance of the approach, we modify our helper function (get_mae) to train and evaluate our model on different datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibyVk-Lrydzx"
   },
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn0fsaerydzx",
    "outputId": "9087b891-109f-4f48-dc77-348e4ca04b1d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mae_imputation = score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid)\n",
    "print(\"MAE using Imputation: {}\".format(mae_imputation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLGPOzt4ydzx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced Imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMWKJ4cQydzx"
   },
   "source": [
    "We see that the imputation approach performs much better compared to the simple solution dropping all rows with NA values.\n",
    "\n",
    "Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1JwWI0Dydzx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the advanced imputation approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we __add a new column__ that shows the __location of the imputed entries__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "2Qsl_hBhydzx",
    "outputId": "408e109a-9f76-4754-df19-b3b587c566e9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original datasets to avoid changing the original data frame\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Find all columns with missing values:\n",
    "cols_with_missing = X_train.columns.values[X_train.isna().sum() > 0]\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "X_train_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Fje9FCrydzx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imputation\n",
    "simple_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(simple_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(simple_imputer.transform(X_valid_plus))\n",
    "\n",
    "# \"Repair\" column names\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd5Zc-3Qydzx",
    "outputId": "eaca794c-d4be-4ef0-a6c9-50ab3c6e8f9e",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mae_imputation_advanced = score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "print(\"MAE using Imputation: {}\".format(mae_imputation_advanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70nKJghmydzx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we see, advanced imputation does not improve the performance of our model in the problem at hand. In general, advanced imputation will meaningfully improve results in some cases. In other cases, it doesn't help at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization, Standardization, Statistical Transformations\n",
    "\n",
    "Our dataset may contain attributes with a mixture of scales for various quantities. However, many machine learning methods require the data attributes to have the same scale. \n",
    "For example, ``yearBuilt`` is measured in years in our dataset at hand whereas the number of rooms obviously has a much smaller scale.\n",
    "To avoid having numeric values from different scales we can use two popular data scaling methods: normalization and standardization.\n",
    "We will implement this step on top of the already imputed dataset from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization refers to rescaling numeric attributes into the __range between 0 and 1.__ It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression.\n",
    "\n",
    "Using Scikit-learn's ``MinMaxScaler`` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - min(X))}{(max(X) - min(X))}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "train_X_normalized = pd.DataFrame(scaler.fit_transform(imputed_X_train), \n",
    "                                      columns=imputed_X_train.columns, index=imputed_X_train.index)\n",
    "val_X_normalized = pd.DataFrame(scaler.transform(imputed_X_valid), \n",
    "                                    columns=imputed_X_valid.columns, index=imputed_X_valid.index)\n",
    "\n",
    "train_X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standardization\n",
    "\n",
    "In contrast to normalization, we could also use standardization for our numerical variables. In this context, standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one. It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes.\n",
    "Using Scikit-learn's ```StandardScaler``` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - mean(X))}{\\sqrt{var(X)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X_standardized = pd.DataFrame(scaler.fit_transform(imputed_X_train), \n",
    "                                        columns=imputed_X_train.columns, index=imputed_X_train.index)\n",
    "val_X_standardized = pd.DataFrame(scaler.transform(imputed_X_valid), \n",
    "                                      columns=imputed_X_valid.columns, index=imputed_X_valid.index)\n",
    "\n",
    "train_X_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log transforms\n",
    "\n",
    "Variables can, such as ``Landsize``, span several orders of magnitude. While the vast majority of persons has small sizes of land, a few people have very vast properties. To work with such skewed variables we can use the log transformation. \n",
    "\n",
    "__Log transforms__ are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. This tends to make the skewed distribution as normal-like as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Landsize'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X_logGains = imputed_X_train.copy()\n",
    "val_X_logGains = imputed_X_valid.copy()\n",
    "\n",
    "\n",
    "\n",
    "train_X_logGains['logLandsize'] = np.log1p(train_X_logGains['Landsize'])\n",
    "val_X_logGains['logLandsize'] = np.log1p(val_X_logGains['Landsize'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see this effect plotting both histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_X_logGains[['Landsize', 'logLandsize']].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Data rescaling is an important part of data preparation before applying machine learning algorithms. However, it is hard to know whether normalization or standardization of the data will improve the performance of a predictive model in advance. \n",
    "\n",
    "A good tip for a practical application is to create rescaled copies of your dataset and evaluate them against each other. This process can quickly show which rescaling method will improve your selected models in the problem at hand. Note that you could (and should) combine these techniques to train powerful models and apply them in real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "We can train our model using the different datasets and feature engineering techniques to evaluate their impact on the model performance. \n",
    "Print the score for the different methods we just introduced and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RhLgR3Rydzx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Categorical Variables\n",
    "\n",
    "Until now we only used numerical features for our models. However, valuable information is often encoded in categorical variables (e.g., gender, city, job).\n",
    "\n",
    "If we simply __plug these categorial variables__ into machine learning models we will get an __error__. Hence, we need to find an appropriate preprocessing to capture the information hidden in categorical variables.\n",
    "\n",
    "The easiest approach to deal with categorical variables is to __drop__ them from the dataset (that is what we have done before). However, this approach will only produce satisfying results if the dropped columns __did not contain useful information__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4ACIFzTydzx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Label Encoding\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/03/label.png\" style=\"width:60%; float:left;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One common approach to handle categorical variables is called label encoding. Here, we assign __each unique value to a different integer (e.g., bad = 0, neutral = 1, good = 2)__.\n",
    "\n",
    "This assumption makes sense in this example, because there is an indisputable __ranking__ to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.\n",
    "\n",
    "For simplicity, we will drop columns with missing values for the following evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwkH4XtTydzx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# Drop NA\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tvafv1Bydzy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we do not want to use all categorical variables we focus on those with a __limited number of categories__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si8WANWSydzy",
    "outputId": "2faf0aab-0e47-42fa-80c9-c8d3c9c41628"
   },
   "outputs": [],
   "source": [
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and\n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "print(low_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHh0Gsubydzy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and combine them with the numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "-SJUmZ1nydzy",
    "outputId": "a8d13e0e-cd6b-4dcd-da05-05219c6ac195"
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep only selected columns\n",
    "cols_to_keep = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[cols_to_keep].copy()\n",
    "X_valid = X_valid_full[cols_to_keep].copy()\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsGhM4-Aydzy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now perform label encoding on our new dataset using the functions provided by Scikit-learn. Subsequently, we can evaluate our approach by using our score_dataset utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JC-wLdTydzy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "oKxsT0Csydzy",
    "outputId": "057dbf69-062f-40ef-84a3-bd40569dae06",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy to protect original data\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in low_cardinality_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "label_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UV6KKrfgydzy",
    "outputId": "3e4f3fdc-65c2-4df4-9232-b8c5c433b8a8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "mae_label_encoding = score_dataset(label_X_train, label_X_valid, y_train, y_valid)\n",
    "print(\"MAE using Label Encoding: {}\".format(mae_label_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5UJrITdydzy",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This result is considerably better than the model that relied only on numerical variables without imputation (MAE 191.669 USD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzvVTZrXydzy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/03/onehot.png\" style=\"width:70%; float:left;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding creates new binary columns indicating the presence (or absence) of each possible value in the original data.\n",
    "\n",
    "In contrast to label encoding, one-hot encoding does __not assume an ordering__ of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data. We refer to categorical variables without an intrinsic ranking as __nominal variables__.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally __won't use__ it for variables taking on __many__ (e.g. more than 15) __different values__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9dWvbPXydzy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Again, we can use Scikit-learn functions to implement one-hot encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PejeyYsGydzz"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMoG_niQydzz",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's first look at the original values of the categorial variables before their transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "6wfkS7ciydzz",
    "outputId": "98ac532b-0efa-49cd-dca3-e985fad163cb"
   },
   "outputs": [],
   "source": [
    "X_train[low_cardinality_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdikkdw-ydzz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "one_hot_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "one_hot_cols_valid = pd.DataFrame(one_hot_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# Repair index\n",
    "one_hot_cols_train.index = X_train.index\n",
    "one_hot_cols_valid.index = X_valid.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J0jcfqdydzz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the result of the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "noPhugaoydzz",
    "outputId": "60dfd90f-0870-4706-c6bc-bda72c2fd88c"
   },
   "outputs": [],
   "source": [
    "one_hot_cols_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_4ubV9eydzz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hD_LuPWFydzz",
    "outputId": "93f423ff-8ac5-46c3-8d01-35b942c42106",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove categorical columns and replace with one-hot encoding\n",
    "num_X_train = X_train.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n",
    "one_hot_X_train = pd.concat([num_X_train, one_hot_cols_train], axis=1)\n",
    "one_hot_X_valid = pd.concat([num_X_valid, one_hot_cols_valid], axis=1)\n",
    "\n",
    "print(one_hot_X_train.head())\n",
    "# Evaluate performance\n",
    "one_hot_encoding = score_dataset(one_hot_X_train.to_numpy(), one_hot_X_valid.to_numpy(), y_train, y_valid)\n",
    "print(\"MAE using One-hot Encoding: {}\".format(one_hot_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhzU8Lh7ydzz",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a small improvement in comparison to the label encoded data (MAE 181.607 USD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LoW_eM1ydzz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "Use standardized numeric features like in the previous section and combine it with OneHot Encoding and Label Encoding for training the model.\n",
    "\n",
    "The final results should have two cases: \n",
    "\n",
    "- One-hot encoded categorical + standardized numeric\n",
    "- Label encoded categorical + standardized numeric\n",
    "\n",
    "For each of the considered categorical columns, does it make more sense to use label encoding or one-hot encoding from a data perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j001C2_Wydz0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Model Pipelines\n",
    "\n",
    "Up to now, we learned how to prepare our datasets, train, tune, and evaluate powerful models. However, we wrote lots of code and functions to perform all the required tasks. Scikit-learn pipelines are a simple way to keep our data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so we can use the whole bundle as if it were a single step.\n",
    "\n",
    "Using pipelines provides multiple benefits:\n",
    "* Cleaner Code\n",
    "* Fewer Bugs\n",
    "* Easier to Productionize\n",
    "* More Options for Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3SWKyHpydz0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will build a pipeline using all numerical variables as well as the low cardinatlity categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "Rsdx2qxWydz0",
    "outputId": "c7fd435a-56e1-4b9c-a14c-5046f0e789d5"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data['Price']\n",
    "X = data[cols_to_keep]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp0jZKltydz0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Writing a pipeline in Scikit-learn can be broken down into 3 steps:\n",
    "1. Define preprocessing steps\n",
    "2. Define the model\n",
    "3. Create and evaluate the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "843YlGhsydz0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Preprocessing steps\n",
    "\n",
    "We use the ``ColumnTransformer`` class to bundle together different preprocessing steps. To this end, we will impute missing values in the numerical columns and impute missing values and use one-hot encoding in the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmaBE-7fydz0"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80H4DOdXydz1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing numerical columns\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Preprocessing categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle both preprocessors\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, low_cardinality_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn3FndA5ydz1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the Model\n",
    "Next, we define a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfNMudLbydz1"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNCy8Utdydz1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create and Evaluate the Pipeline\n",
    "\n",
    "Finally, we use the ``Pipeline`` class to define a pipeline that bundles the preprocessing and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQPwGodBydz1",
    "outputId": "a1534fdb-fa7d-4f63-bdbe-2537e9ea2f64"
   },
   "outputs": [],
   "source": [
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "complete_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Preprocess the raw training data and fit the model\n",
    "complete_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess the raw validation data and make predictions\n",
    "preds = complete_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print(\"MAE using the complete pipeline: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6mSGM8Jydz1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " There are a few important things to notice:\n",
    "\n",
    "* With the pipeline, we preprocess the training data and fit the model in a __single line of code__. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "* With the pipeline, we supply the __unprocessed features in X_valid to the predict()__ command, and the pipeline __automatically preprocesses__ the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 5\n",
    "\n",
    "Extend the pipeline by including the normalization step for the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "K-Means clustering is the __most popular unsupervised__ learning algorithm. It is used when we have unlabelled data which is data without defined categories or groups. The algorithm follows an easy or simple way to classify a given data set through a certain number of __clusters, fixed apriori.__ K-Means algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on __feature similarity.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/kmeans.png\" style=\"width:80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goal\n",
    "\n",
    "Given a __dataset__ and a __number of clusters *k*__, K-Means tries to:\n",
    "\n",
    "- Partition the data into *k* groups\n",
    "- Minimize the **within-cluster sum of squared distances** (WCSS)\n",
    "\n",
    "In other words:\n",
    "> Find cluster centers so that points within a cluster are as close as possible to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The K-Means Algorithm\n",
    "\n",
    "1. **Initialization**  \n",
    "   Choose *k* initial cluster centers (centroids), either randomly or using k-means++.\n",
    "\n",
    "2. **Assignment Step**  \n",
    "   For each data point:\n",
    "   - Assign it to the nearest centroid (measured by Euclidean distance).\n",
    "\n",
    "3. **Update Step**  \n",
    "   For each cluster:\n",
    "   - Recompute the centroid as the **mean** of all points assigned to it.\n",
    "\n",
    "4. **Repeat**  \n",
    "   - Alternate between *assignment* and *update* steps  \n",
    "   - Stop when centroids no longer change significantly (convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means Algorithm\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/kmeans_algorithm.png\" style=\"width:50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/vhaus63/ids_data/main/Mall_Customers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Annual Income (k$)', 'Spending Score (1-100)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from lets_plot import *\n",
    "\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "plt = (\n",
    "    ggplot(X, aes(x='Annual Income (k$)', y='Spending Score (1-100)'))\n",
    "    + geom_point()\n",
    "    + ggsize(700, 400)\n",
    "    + labs(\n",
    "        x='Annual Income (k$)',\n",
    "        y='Spending Score (1-100)',\n",
    "        title='Scatter Plot of Annual Income vs. Spending Score'\n",
    "    )\n",
    "    + theme_minimal()\n",
    ")\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can fit the model. We choose 5 clusters for the beginning, based on our visual inspection of the dataset and its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\")\n",
    "kmeans.fit(X)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the cluster membership for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "clusters_df = pd.concat([X, pd.Series(kmeans.labels_, name='Cluster')], axis=1)\n",
    "\n",
    "# Scatter plot for clusters\n",
    "p = ggplot(clusters_df) + \\\n",
    "    geom_point(aes(x='Annual Income (k$)', y='Spending Score (1-100)', color='Cluster')) + \\\n",
    "    scale_color_discrete(name='Cluster') + \\\n",
    "    labs(x='Annual Income (k$)', y='Spending Score (1-100)', color='Cluster')\n",
    "\n",
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters to tune the model\n",
    "\n",
    "#### 1. k - the amount of clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can estimate a suitable number of clusters __using inertia__ and the __elbow method:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__WCSS (Within-Cluster Sum of Squares),__ also known as __inertia,__ measures the sum of squared distances between each data point and the centroid of its assigned cluster. The centroid is the mean position of all the points in the cluster.\n",
    "It provides a measure of the \"costs\" of the k means model and can help to identify the right choice of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def elbow_method(X, max_k = 10):\n",
    "    costs = []\n",
    "    for k in range(2, max_k):\n",
    "        model = KMeans(n_clusters=k, random_state=0, n_init=\"auto\")\n",
    "        model.fit(X)\n",
    "        costs.append(model.inertia_)\n",
    "\n",
    "    costs_df = pd.DataFrame({\n",
    "        'K': range(2, max_k),  # Cluster counts (k)\n",
    "        'Cost': costs  # The corresponding cost (inertia)\n",
    "    })\n",
    "    \n",
    "    # Plotting the elbow curve\n",
    "    p = (ggplot(costs_df, aes(x='K', y='Cost'))\n",
    "        + geom_line()\n",
    "        + geom_point()\n",
    "        + labs(x='Number of Clusters (k)', y='Cost (Inertia)'))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mentimeter (Elbow method)\n",
    "\n",
    "How many clusters to choose based on the following plot? Answer on [Mentimeter](https://www.menti.com/alurte2x2dno). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "elbow_method(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The elbow method should be utilized with care since it is both __subjective and unreliable.__ In many practical applications, the choice of an \"elbow\" is highly ambiguous as the plot does not contain a sharp elbow. Because the two axes (the number of clusters and the remaining variance) have no semantic relationship, the plot is sensitive to the __parameter range.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. The distance measure\n",
    "\n",
    "The choice of a distance metric can affect the clustering results, depending on the nature of the data.\n",
    "\n",
    "Some distance measures are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Euclidean Distance**: $d(p,q)=\\sqrt(\\sum_{i=1}^n(p_i-q_i)^2)$.\n",
    "\n",
    "It is the most widely used distance metric that reflects our __intuitive__ way of understanding distances and works well with __spherical clusters__ in continuous spaces. However, it is __sensitive to scale,__ so features with a wide range can dominate the clustering results, which might not always be desired. It also assumes some circle-like underlying structure of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Manhattan distance**: $d(p,q)=\\sum_{i=1}^n\\vert p_i-q_i\\vert$.\n",
    "\n",
    "The Manhattan distance (or L1 Norm) handles different scales in features better than the euclidean distance (in contrast to which no square is introduced) and is therefore __less affected from outliers.__ It works very well on grid-like data. This also means, that it assumes an alignment of the clusters with the axes: it is __not invariant under rotation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Hamming Distance**: Number of positions where $p_i\\neq q_i$.\n",
    "\n",
    "The Hamming distance is an effective and simple metric for categorical data. However, due to its definition, it is not applicable to continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Chi-Squared Distance**: $d(p,q)=\\sum_{i=1}^n\\frac{(p_i-q_i)^2}{p_i+q_i}$, where $p_i+q_i>0$.\n",
    "\n",
    "It is commonly used in clustering tasks, particularly for histograms and frequency data. As visible from the formula, it accounts for the relative difference between features. It is well-suited for __comparing distributions or normalized data.__ The Chi-Squared distance can only be applied to non-negative data and it is very sensitive to small values. Additionally, like the Manhattan Distance, it is not rotation invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Excursus: Effect of different distance metrics (not relevant for exam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sklearns' `kMeans` does not natively support the use of other distance metrices than the euclidean distance. To demonstrate the effect of changing this measure, we will use the `PyClustering` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyclustering.cluster.kmeans import kmeans\n",
    "from pyclustering.cluster.center_initializer import random_center_initializer\n",
    "from pyclustering.utils import distance_metric, type_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_list = [[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0], [10, 2], [10, 4], [10, 0]]\n",
    "\n",
    "metrics = [\n",
    "    ('Squared Euclidean', distance_metric(type_metric.EUCLIDEAN_SQUARE)),\n",
    "    ('Chi Square', distance_metric(type_metric.CHI_SQUARE)),\n",
    "    ('Manhattan', distance_metric(type_metric.MANHATTAN))\n",
    "]\n",
    "\n",
    "for metric_name, metric in metrics:\n",
    "    print(f\"Clustering with {metric_name} Distance:\")\n",
    "\n",
    "    initial_centers = random_center_initializer(X_list, 5, random_state=0).initialize()\n",
    "\n",
    "    kmeans_instance = kmeans(X_list, initial_centers=initial_centers, metric=metric)\n",
    "    kmeans_instance.process()\n",
    "    \n",
    "    centers = kmeans_instance.get_centers()\n",
    "    clusters = kmeans_instance.get_clusters()\n",
    "    \n",
    "    print(f\"  Cluster Centers: {centers}\")\n",
    "    print(f\"  Clusters: {clusters}\\n\")\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point_idx in cluster:\n",
    "            x, y = X_list[point_idx]\n",
    "            cluster_data.append({'x': x, 'y': y, 'label': cluster_idx})\n",
    "    center_data = [{'x': center[0], 'y': center[1], 'label': f'C{idx}'} for idx, center in enumerate(centers)]\n",
    "    \n",
    "    # Convert to dataframes for plotting\n",
    "    cluster_df = pd.DataFrame(cluster_data)\n",
    "    center_df = pd.DataFrame(center_data)\n",
    "    \n",
    "    # Plot clusters and centers\n",
    "    plot = (ggplot(cluster_df, aes('x', 'y', shape='label',  color='label'))\n",
    "            + scale_x_continuous(breaks = np.arange(0, 11,  1))\n",
    "            + scale_y_continuous(breaks = np.arange(0, 10,  1))\n",
    "            + geom_point(size=8)\n",
    "            + scale_shape(name='Cluster')\n",
    "            + coord_cartesian(ylim=(-1, 7), xlim = (0,11)) \n",
    "            + scale_color_discrete(name='Cluster')\n",
    "            + ggtitle(f'Clusters using {metric_name} Distance')            \n",
    "            + theme_minimal()\n",
    "           )\n",
    "    display(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Choosing the right distance metric__ is an important step in using clustering algorithms like KMeans. The choice depends on the type of data you're working with-__continuous, categorical, or binary__, its inherent structure and relationships within your data and the specific requirements of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More on k-means clustering\n",
    "\n",
    "- For an alternative explanation of k-means, please watch [this video](https://www.youtube.com/watch?v=4b5d3muPQmA). <span style=\"color:red\">__This content is relevant for the exam!__</span>\n",
    "- For a formal coverage see [Hastie et al: Elements of Statistical Learning, Chapter 13](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition to the k-Means clustering algorithm, there are several other options to choose from. Here is an overview on how they deal with different datsets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/different_clustering_algos.png\" style=\"width:70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can see very well here that kMeans performs very well on __spherical clusters,__ but __not at all on non-convex clusters.__ For these shapes, other algorithms like DBSCAN are recommendet. If you are interested in other clustering algorithms, please refer to the [documentation](https://scikit-learn.org/1.5/auto_examples/cluster/plot_cluster_comparison.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Let's summarize the clustering using [Mentimeter](https://www.menti.com/alurte2x2dno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Next lecture: Descriptive Models!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/ao_modeling.png\" style=\"width:80%\" />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "rise": {
   "custom_css": "./rise.css",
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>Introduction to Data Science (IDS)</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
